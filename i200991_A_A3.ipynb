{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNT6OO0o65kW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')"
      ],
      "metadata": {
        "id": "rp8Bi_Mm-l7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "uBaQgzYS-7Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 14, 8\n",
        "RANDOM_SEED = 42\n",
        "LABELS = [\"Normal\", \"Fraud\"]"
      ],
      "metadata": {
        "id": "pVY8lmTq--Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "xXOdTSvr_Ai1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "NRXiDXMJ_Crf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "vF5hnXbk_EjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_classes = pd.value_counts(data['Class'], sort = True)\n",
        "\n",
        "count_classes.plot(kind = 'bar', rot=0)\n",
        "\n",
        "plt.title(\"Transaction Class Distribution\")\n",
        "\n",
        "plt.xticks(range(2), LABELS)\n",
        "\n",
        "plt.xlabel(\"Class\")\n",
        "\n",
        "plt.ylabel(\"Frequency\")"
      ],
      "metadata": {
        "id": "L9vVzNRp_JQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud= data[data['Class']==1]\n",
        "normal = data[data['Class']==0]\n",
        "print(fraud.shape)\n",
        "print(normal.shape)"
      ],
      "metadata": {
        "id": "IUyt_JHj_L8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud.columns"
      ],
      "metadata": {
        "id": "12q3NRDw_NxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "data['NormalizedAmount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
        "data.drop(['Time', 'Amount'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "bDxzjXuH_auW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Au5i54Wq_bft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "class Generator(Model):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        # Define your layers here\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Implement the forward pass of your generator\n",
        "        return outputs\n",
        "\n",
        "class Discriminator(Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Define your layers here\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Implement the forward pass of your discriminator\n",
        "        return outputs\n",
        "\n",
        "# Contrastive Learning\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    # Implement contrastive loss function\n",
        "    return loss\n",
        "\n",
        "# Training\n",
        "def train_model(x_train, epochs=10, batch_size=32):\n",
        "    generator = Generator()\n",
        "    discriminator = Discriminator()\n",
        "\n",
        "    # Compile discriminator\n",
        "    discriminator.compile(loss=contrastive_loss, optimizer=Adam())\n",
        "\n",
        "    # Train discriminator\n",
        "    discriminator.fit(x_train, np.zeros((len(x_train), 1)), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    # Compile generator\n",
        "    discriminator.trainable = False\n",
        "    gan = Model(generator.input, discriminator(generator.output))\n",
        "    gan.compile(loss=contrastive_loss, optimizer=Adam())\n",
        "\n",
        "    # Train GAN\n",
        "    gan.fit(x_train, np.ones((len(x_train), 1)), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    return generator\n",
        "\n",
        "# Anomaly Detection\n",
        "def detect_anomalies(model, x_test):\n",
        "    # Implement anomaly detection using the trained model\n",
        "    return anomalies\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(anomalies, y_test):\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, anomalies, average='binary')\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(x_train.drop('Class', axis=1), epochs=10, batch_size=32)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies = detect_anomalies(trained_model, x_test.drop('Class', axis=1))\n",
        "\n",
        "# Evaluate the model\n",
        "precision, recall, f1_score = evaluate_model(anomalies, x_test['Class'])\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1_score)"
      ],
      "metadata": {
        "id": "Zkt635Uv_du0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}